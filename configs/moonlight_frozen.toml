# Moonlight with frozen experts from moonshotai/Moonlight-16B-A3B
#
# Usage:
#   1. Download frozen weights:
#      python -m scripts.download_experts --model-id moonshotai/Moonlight-16B-A3B \
#        --output /data/moonlight_frozen --n-dense-layers 1 --n-layers 27 --n-experts 64
#   2. Initialize embeddings:
#      python -m scripts.init_embeddings --model-id moonshotai/Moonlight-16B-A3B \
#        --output /data/moonlight_frozen/embeddings.pt
#   3. Initialize model:
#      python -m scripts.init_frozen_model --config configs/moonlight_frozen.toml --verify
#   4. Train:
#      torchrun --nproc_per_node=8 -m nmoe.train configs/moonlight_frozen.toml

preset = "moonlight_frozen"
experiment_id = "moonlight_frozen_dev"

# =============================================================================
# Model Architecture (matches moonshotai/Moonlight-16B-A3B)
# =============================================================================
vocab_size = 201088         # o200k_harmony tokenizer (MATT-initialized from teacher)
tokenizer  = "o200k_harmony"
dim = 2048
inter_dim = 11264
moe_inter_dim = 1408
n_layers = 27
n_dense_layers = 1          # Layer 0 is dense, layers 1-26 are MoE
n_heads = 16
n_routed_experts = 64
n_shared_experts = 2
n_activated_experts = 6
route_scale = 2.446

# Attention pattern (MLA/SWA): configured in code via `attn_*` nested dicts.

# =============================================================================
# Frozen Expert Configuration
# =============================================================================
frozen_experts = true
frozen_dir = "/data/moonlight_frozen"
dtype = "nvfp4"

# =============================================================================
# Data
# =============================================================================
data_path = "/data/fineweb_edu"

# =============================================================================
# Optimizer (only for trainable params: attention, embedding, lm_head, router, norms)
# =============================================================================
lr_dense = 3e-3     # Attention projections (random init, full training)
lr_router = 1e-5    # Router gate (teacher init, preserve routing)
lr_embed = 5e-5     # Embedding/lm_head (MATT init, fine-tune)

# =============================================================================
# Training (batch=256 for 20K steps = 21B tokens)
# =============================================================================
steps = 20000
batch_size = 256     # 8 GPUs: 32 seq/GPU (enabled by fused gather+quant and scatter optimizations)
seq_len = 4096
resume = false
checkpoint_every = 1000
eval_every = 500
experiments_db = "/data/experiments_moonlight.db"
